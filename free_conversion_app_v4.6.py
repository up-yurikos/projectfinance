import streamlit as st
import pandas as pd
import zipfile, io, re, tempfile, gdown, calendar
from datetime import date, datetime
from dateutil.relativedelta import relativedelta

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒšãƒ¼ã‚¸è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåç›Š v7.1", layout="wide")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def detect_col(cols, patterns):
    for p in patterns:
        hits = [c for c in cols if re.sub(r"\s+", "", str(c)).casefold() == p]
        if hits:
            return hits[0]
    return None


def normalize_id(x):
    if pd.isna(x):
        return None
    s = str(x).strip()
    return None if s == "" or s.lower() in {"nan", "none"} else s


# â”€â”€ CSV / ZIP ãƒ­ãƒ¼ãƒ€ ---------------------------------------------------------
def load_csv(uploaded):
    """â‘  CSV â‘¡ ZIP å†… CSV ã‚’ DataFrame ã§è¿”ã™"""
    if uploaded is None:
        return None

    # ZIP
    if uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded) as zf:
                csv_files = [n for n in zf.namelist()
                             if n.lower().endswith(".csv") and not n.endswith("/")]
                if not csv_files:
                    st.error("ZIP ã« CSV ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    return None
                target = csv_files[0] if len(csv_files) == 1 else \
                         st.selectbox("ZIP å†… CSV ã‚’é¸æŠã—ã¦ãã ã•ã„", csv_files)
                with zf.open(target) as fp:
                    for enc in ("utf-8-sig", "cp932", "utf-8"):
                        try:
                            txt = fp.read().decode(enc)
                            return pd.read_csv(io.StringIO(txt))
                        except Exception:
                            fp.seek(0)
        except zipfile.BadZipFile:
            st.error("ZIP ãƒ•ã‚¡ã‚¤ãƒ«ãŒå£Šã‚Œã¦ã„ã¾ã™ã€‚")
            return None

    # é€šå¸¸ CSV
    for enc in ("utf-8-sig", "cp932", "utf-8"):
        try:
            txt = uploaded.read().decode(enc)
            return pd.read_csv(io.StringIO(txt))
        except Exception:
            uploaded.seek(0)

    st.error("CSV ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    return None


# â”€â”€ Google Drive å…±æœ‰ãƒªãƒ³ã‚¯ ---------------------------------------------------
def read_gdrive_csv_gdown(url: str, **kw) -> pd.DataFrame:
    """å…±æœ‰ãƒªãƒ³ã‚¯ â†’ gdown ã§ DL â†’ DataFrame"""
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp:
        gdown.download(url=url, output=tmp.name, quiet=False, fuzzy=True)
        return pd.read_csv(tmp.name, **kw)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ã‚µã‚¤ãƒ‰ãƒãƒ¼ : ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ï¼ˆExpanderï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar.expander("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ / é¸æŠ", expanded=True):
    # ä»•è¨³å¸³
    tab_local, tab_drive = st.tabs(["ãƒ­ãƒ¼ã‚«ãƒ« CSV / ZIP", "Google Drive å…±æœ‰ãƒªãƒ³ã‚¯"])

    with tab_local:
        uploaded_file = st.file_uploader("ä»•è¨³å¸³ (CSV / ZIP)", type=["csv", "zip"])

    with tab_drive:
        gdrive_url = st.text_input("å…±æœ‰ãƒªãƒ³ã‚¯ã‚’è²¼ã£ã¦ Enter",
                                   placeholder="https://drive.google.com/file/d/â€¦/view?usp=sharing")
        if gdrive_url and not gdrive_url.startswith("http"):
            st.warning("ãƒªãƒ³ã‚¯å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
            gdrive_url = ""

    st.markdown("---")

    # å–å¼•ãƒã‚¹ã‚¿ / ç¨¼åƒã‚³ã‚¹ãƒˆ
    master_file = st.file_uploader("å–å¼•ãƒã‚¹ã‚¿ (CSV)", type="csv")
    cost_file   = st.file_uploader("ç¨¼åƒã‚³ã‚¹ãƒˆ (CSV)", type="csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä»•è¨³å¸³èª­è¾¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_src = None
if uploaded_file is not None:
    df_src = load_csv(uploaded_file)
elif gdrive_url:
    try:
        df_src = read_gdrive_csv_gdown(gdrive_url, encoding="cp932")
    except Exception as e:
        st.error(f"Google Drive èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

if df_src is None:
    st.stop()

st.success(f"ä»•è¨³å¸³ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ({len(df_src):,} è¡Œ)")
df_src["å–å¼•æ—¥"] = pd.to_datetime(df_src["å–å¼•æ—¥"], errors="coerce")

# ä»¥é™: å–å¼•ãƒã‚¹ã‚¿ / ç¨¼åƒã‚³ã‚¹ãƒˆ / pivot / ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨ˆç®— ã¯ v7.0 ã¨åŒã˜
# -------------------------------------------------------------------
# å–å¼•ãƒã‚¹ã‚¿èª­è¾¼ -----------------------------------------------------
master_map = None
if master_file:
    df_master = load_csv(master_file)
    if df_master is not None:
        df_master.columns = df_master.columns.map(str.strip)
        id_col   = detect_col(df_master.columns, ["å–å¼•id","ãƒ¬ã‚³ãƒ¼ãƒ‰id","recordid","dealid"])
        amt_col  = detect_col(df_master.columns, ["é‡‘é¡","å£²ä¸Šé‡‘é¡","è¦‹ç©é‡‘é¡","amount"])
        if id_col and amt_col:
            df_master[id_col]  = df_master[id_col].map(normalize_id)
            df_master[amt_col] = pd.to_numeric(df_master[amt_col], errors="coerce").fillna(0)
            keep = {id_col:"å–å¼•ã‚³ãƒ¼ãƒ‰", amt_col:"ãƒã‚¹ã‚¿ãƒ¼é‡‘é¡"}
            # è¿½åŠ åˆ—
            for src, dst in [(["å–å¼•å…ˆ","ä¼šç¤¾å","customer","client"],"ãƒã‚¹ã‚¿ãƒ¼å–å¼•å…ˆ"),
                             (["å–å¼•å","æ¡ˆä»¶å","dealname","title"],"å–å¼•å"),
                             (["å–å¼•æ‹…å½“è€…","æ‹…å½“è€…","owner","sales"],"å–å¼•æ‹…å½“è€…"),
                             (["industry","æ¥­ç•Œ"],"Industry"),
                             (["industryè©³ç´°","industrydetail","æ¥­ç•Œè©³ç´°"],"Industryè©³ç´°")]:
                c = detect_col(df_master.columns, src)
                if c: keep[c] = dst
            master_map = (df_master[list(keep)]
                          .dropna(subset=[id_col])
                          .rename(columns=keep)
                          .drop_duplicates(subset=["å–å¼•ã‚³ãƒ¼ãƒ‰"]))

# ç¨¼åƒã‚³ã‚¹ãƒˆèª­è¾¼ -----------------------------------------------------
df_cost_raw = None
if cost_file:
    df_cost_raw = load_csv(cost_file)        # å¾Œã§ Utilization ã«ä½¿ã†
    if df_cost_raw is not None:
        df_cost_raw.columns = df_cost_raw.columns.map(str.strip)

# -------------- æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã§ daily ã‚’ä½œæˆ (å£²ä¸Šãƒ»è²»ç”¨ãƒ»äººä»¶è²») --------------
# å£²ä¸Šãƒ»è²»ç”¨æ•´å½¢
df_sales = df_src[df_src.get("è²¸æ–¹å‹˜å®šç§‘ç›®") == "å£²ä¸Šé«˜"].copy()
df_sales_out = pd.DataFrame({
    "å–å¼•ã‚³ãƒ¼ãƒ‰": df_sales["è²¸æ–¹éƒ¨é–€"].astype(str).str.strip(),
    "å–å¼•å…ˆ": df_sales["è²¸æ–¹å–å¼•å…ˆå"],
    "å‹˜å®šç§‘ç›®": "å£²ä¸Šé«˜",
    "é‡‘é¡": pd.to_numeric(df_sales["è²¸æ–¹é‡‘é¡"], errors="coerce").fillna(0),
    "æ—¥ä»˜": df_sales["å–å¼•æ—¥"]
})

targets = ["å¤–æ³¨è²»","äº¤éš›è²»","æ—…è²»äº¤é€šè²»"]
df_exp = df_src[df_src.get("å€Ÿæ–¹å‹˜å®šç§‘ç›®").isin(targets)].copy()
df_exp_out = pd.DataFrame({
    "å–å¼•ã‚³ãƒ¼ãƒ‰": df_exp["å€Ÿæ–¹éƒ¨é–€"].astype(str).str.strip(),
    "å–å¼•å…ˆ": df_exp["å€Ÿæ–¹å–å¼•å…ˆå"],
    "å‹˜å®šç§‘ç›®": df_exp["å€Ÿæ–¹å‹˜å®šç§‘ç›®"],
    "é‡‘é¡": pd.to_numeric(df_exp["å€Ÿæ–¹é‡‘é¡"], errors="coerce").fillna(0),
    "æ—¥ä»˜": df_exp["å–å¼•æ—¥"]
})
combo = pd.concat([df_sales_out, df_exp_out], ignore_index=True)

# pivot
daily = (combo.pivot_table(index="å–å¼•ã‚³ãƒ¼ãƒ‰", columns="å‹˜å®šç§‘ç›®",
                           values="é‡‘é¡", aggfunc="sum",
                           fill_value=0)
         .reset_index())
meta = (combo.groupby("å–å¼•ã‚³ãƒ¼ãƒ‰")
             .agg({"æ—¥ä»˜":["min","max"],"å–å¼•å…ˆ":"first"}).reset_index())
meta.columns = ["å–å¼•ã‚³ãƒ¼ãƒ‰","æ—¥ä»˜ï¼ˆæœ€å°ï¼‰","æ—¥ä»˜ï¼ˆæœ€å¤§ï¼‰","å–å¼•å…ˆ"]
daily = daily.merge(meta, on="å–å¼•ã‚³ãƒ¼ãƒ‰", how="left")
if "å£²ä¸Šé«˜" not in daily.columns:
    daily["å£²ä¸Šé«˜"] = 0

# äººä»¶è²» (ç¨¼åƒã‚³ã‚¹ãƒˆ CSV) ------------------------------------------------------
if df_cost_raw is not None:
    id_c   = detect_col(df_cost_raw.columns, ["å–å¼•id","ãƒ¬ã‚³ãƒ¼ãƒ‰id","recordid"])
    cost_c = detect_col(df_cost_raw.columns, ["ç¨¼åƒã‚³ã‚¹ãƒˆ","äººä»¶è²»","cost"])
    name_c = detect_col(df_cost_raw.columns, ["ä¼šç¤¾å","å–å¼•å…ˆ","client","customer"])
    if id_c and cost_c:
        df_cost_raw[id_c] = df_cost_raw[id_c].map(normalize_id)
        df_cost = df_cost_raw.dropna(subset=[id_c]).copy()
        df_cost["äººä»¶è²»"] = pd.to_numeric(df_cost[cost_c], errors="coerce").fillna(0)
        agg = {"äººä»¶è²»":"sum"}
        if name_c: agg[name_c] = "ç¨¼åƒå–å¼•å…ˆ"
        cost_info = (df_cost.groupby(id_c, as_index=False)
                     .agg(agg)
                     .rename(columns={id_c:"å–å¼•ã‚³ãƒ¼ãƒ‰"}))
        daily["å–å¼•ã‚³ãƒ¼ãƒ‰"] = daily["å–å¼•ã‚³ãƒ¼ãƒ‰"].map(normalize_id)
        daily = daily.merge(cost_info, on="å–å¼•ã‚³ãƒ¼ãƒ‰", how="left")
    else:
        daily["äººä»¶è²»"] = 0
else:
    daily["äººä»¶è²»"] = 0

# ãƒã‚¹ã‚¿è£œå®Œ -----------------------------------------------------------
if master_map is not None:
    daily = daily.merge(master_map, on="å–å¼•ã‚³ãƒ¼ãƒ‰", how="left")
    need_fix = daily["å£²ä¸Šé«˜"] == 0
    daily.loc[need_fix, "å£²ä¸Šé«˜"] = daily.loc[need_fix, "ãƒã‚¹ã‚¿ãƒ¼é‡‘é¡"]
    has_cost_name = daily["ç¨¼åƒå–å¼•å…ˆ"].notna()
    daily.loc[need_fix & has_cost_name, "å–å¼•å…ˆ"] = \
        daily.loc[need_fix & has_cost_name, "ç¨¼åƒå–å¼•å…ˆ"]

# æŒ‡æ¨™è¨ˆç®— -------------------------------------------------------------
for c in ["å£²ä¸Šé«˜","å¤–æ³¨è²»","äº¤éš›è²»","æ—…è²»äº¤é€šè²»","äººä»¶è²»"]:
    daily[c] = pd.to_numeric(daily[c], errors="coerce").fillna(0)
daily["æœˆæ•°"] = daily.apply(
    lambda r: max((r["æ—¥ä»˜ï¼ˆæœ€å¤§ï¼‰"].year - r["æ—¥ä»˜ï¼ˆæœ€å°ï¼‰"].year)*12 +
                  (r["æ—¥ä»˜ï¼ˆæœ€å¤§ï¼‰"].month - r["æ—¥ä»˜ï¼ˆæœ€å°ï¼‰"].month) + 1, 1),
    axis=1)
daily["æœˆæ¬¡å£²ä¸Š"] = daily.apply(
    lambda r: r["å£²ä¸Šé«˜"]/r["æœˆæ•°"] if r["æœˆæ•°"] else 0, axis=1)
daily["ç²—åˆ©"] = (daily["å£²ä¸Šé«˜"] - daily["å¤–æ³¨è²»"] - daily["äº¤éš›è²»"]
                 - daily["æ—…è²»äº¤é€šè²»"] - daily["äººä»¶è²»"])
daily["ç²—åˆ©ç‡"] = daily.apply(
    lambda r: (r["ç²—åˆ©"]/r["å£²ä¸Šé«˜"]*100) if r["å£²ä¸Šé«˜"]>0 else 0, axis=1)
daily.rename(columns={"å–å¼•ã‚³ãƒ¼ãƒ‰":"ãƒ¬ã‚³ãƒ¼ãƒ‰ID"}, inplace=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.markdown("### ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š")
id_val = st.sidebar.text_input("å–å¼•IDï¼ˆéƒ¨åˆ†ä¸€è‡´å¯ï¼‰", "")
min_date = df_src["å–å¼•æ—¥"].min().date()
max_date = df_src["å–å¼•æ—¥"].max().date()
start_date, end_date = st.sidebar.date_input("æœŸé–“ç¯„å›²", [min_date, max_date])

owner_sel = ind_sel = ind_det_sel = []
if "å–å¼•æ‹…å½“è€…" in daily.columns:
    owner_sel = st.sidebar.multiselect("å–å¼•æ‹…å½“è€…",
                                       sorted(daily["å–å¼•æ‹…å½“è€…"].dropna().unique()))
if "Industry" in daily.columns:
    ind_sel = st.sidebar.multiselect("Industry",
                                     sorted(daily["Industry"].dropna().unique()))
if "Industryè©³ç´°" in daily.columns:
    ind_det_sel = st.sidebar.multiselect("Industryè©³ç´°",
                                         sorted(daily["Industryè©³ç´°"].dropna().unique()))

mask  = daily["ãƒ¬ã‚³ãƒ¼ãƒ‰ID"].str.contains(id_val, na=False)
mask &= daily["æ—¥ä»˜ï¼ˆæœ€å¤§ï¼‰"].dt.date >= start_date
mask &= daily["æ—¥ä»˜ï¼ˆæœ€å°ï¼‰"].dt.date <= end_date
if owner_sel:
    mask &= daily["å–å¼•æ‹…å½“è€…"].isin(owner_sel)
if ind_sel:
    mask &= daily["Industry"].isin(ind_sel)
if ind_det_sel:
    mask &= daily["Industryè©³ç´°"].isin(ind_det_sel)
df_filtered = daily[mask].copy()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æœˆæ¬¡ãƒ†ãƒ¼ãƒ–ãƒ« & æ¡ˆä»¶æ•°ï¼ˆæœŸé–“å†…ã®ã¿ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
records_sales, records_profit = [], []
for _, r in df_filtered.iterrows():
    for i in range(int(r["æœˆæ•°"])):
        dt_m = r["æ—¥ä»˜ï¼ˆæœ€å°ï¼‰"] + relativedelta(months=i)
        if not (start_date <= dt_m.date() <= end_date):
            continue
        ym = dt_m.strftime("%y/%m")
        records_sales.append({"å¹´æœˆè¡¨ç¤º":ym,"ãƒ¬ã‚³ãƒ¼ãƒ‰ID":r["ãƒ¬ã‚³ãƒ¼ãƒ‰ID"],
                              "å–å¼•å…ˆ":r["å–å¼•å…ˆ"],
                              "æœˆæ¬¡å£²ä¸Š":round(r["æœˆæ¬¡å£²ä¸Š"])})
        records_profit.append({"å¹´æœˆè¡¨ç¤º":ym,"ãƒ¬ã‚³ãƒ¼ãƒ‰ID":r["ãƒ¬ã‚³ãƒ¼ãƒ‰ID"],
                               "å–å¼•å…ˆ":r["å–å¼•å…ˆ"],
                               "æœˆæ¬¡ç²—åˆ©":round(r["ç²—åˆ©"]/r["æœˆæ•°"]) if r["æœˆæ•°"] else 0})
df_ms = pd.DataFrame(records_sales)
df_mp = pd.DataFrame(records_profit)

df_sales_p  = (df_ms.pivot_table(index=["ãƒ¬ã‚³ãƒ¼ãƒ‰ID","å–å¼•å…ˆ"],
                                 columns="å¹´æœˆè¡¨ç¤º",
                                 values="æœˆæ¬¡å£²ä¸Š", fill_value=0).reset_index())
df_profit_p = (df_mp.pivot_table(index=["ãƒ¬ã‚³ãƒ¼ãƒ‰ID","å–å¼•å…ˆ"],
                                 columns="å¹´æœˆè¡¨ç¤º",
                                 values="æœˆæ¬¡ç²—åˆ©", fill_value=0).reset_index())
time_cols = sorted(df_ms["å¹´æœˆè¡¨ç¤º"].unique().tolist())

summary_sales = pd.DataFrame({
    "ãƒ¬ã‚³ãƒ¼ãƒ‰ID":["",""], "å–å¼•å…ˆ":["â‘ æœˆæ¬¡å£²ä¸Šåˆè¨ˆ","â‘¡å¹³å‡å£²ä¸Šå˜ä¾¡"],
    **{c:[df_sales_p[c].sum(), df_sales_p[c].mean()] for c in time_cols}
})
summary_profit = pd.DataFrame({
    "ãƒ¬ã‚³ãƒ¼ãƒ‰ID":["",""], "å–å¼•å…ˆ":["â‘¢æœˆæ¬¡ç²—åˆ©åˆè¨ˆ","â‘£å¹³å‡ç²—åˆ©å˜ä¾¡"],
    **{c:[df_profit_p[c].sum(), df_profit_p[c].mean()] for c in time_cols}
})
for c in time_cols:
    summary_sales[c]  = summary_sales[c].map(lambda x:f"{int(x):,}")
    summary_profit[c] = summary_profit[c].map(lambda x:f"{int(x):,}")

count_series = (df_ms[df_ms["æœˆæ¬¡å£²ä¸Š"]>0]
                .groupby("å¹´æœˆè¡¨ç¤º")["ãƒ¬ã‚³ãƒ¼ãƒ‰ID"].nunique()
                .reindex(time_cols, fill_value=0))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Utilization è¨ˆç®—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
util_hours_pivot = pd.DataFrame()
util_pct_pivot   = pd.DataFrame()
std_hours_row    = {}

if df_cost_raw is not None:
    # å¯¾è±¡åˆ—
    cons_c  = detect_col(df_cost_raw.columns, ["ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ","æ°å","name"])
    hours_c = detect_col(df_cost_raw.columns, ["ç¨¼åƒæ™‚é–“","hours","workinghours","h"])
    date_c  = detect_col(df_cost_raw.columns, ["æ—¥ä»˜","ç¨¼åƒæ—¥","date"])
    if cons_c and hours_c and date_c:
        dc = df_cost_raw[[cons_c, hours_c, date_c]].copy()
        dc[date_c] = pd.to_datetime(dc[date_c], errors="coerce")
        dc["å¹´æœˆè¡¨ç¤º"] = dc[date_c].dt.strftime("%y/%m")
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æœŸé–“
        dc = dc[(dc[date_c].dt.date >= start_date) &
                (dc[date_c].dt.date <= end_date)]
        # æœˆæ¬¡ç¨¼åƒæ™‚é–“
        util_hours  = (dc.pivot_table(index=cons_c, columns="å¹´æœˆè¡¨ç¤º",
                                      values=hours_c, aggfunc="sum",
                                      fill_value=0)
                       .reindex(columns=time_cols, fill_value=0))
        util_hours_pivot = util_hours.reset_index()
        # æ¨™æº–ç¨¼åƒæ™‚é–“
        for col in time_cols:
            dt = datetime.strptime(col, "%y/%m")
            _, last_day = calendar.monthrange(dt.year, dt.month)
            wd = pd.date_range(dt.strftime("%Y-%m-01"),
                               dt.strftime(f"%Y-%m-{last_day}"),
                               freq="B").size
            std_hours_row[col] = wd * 8
        # ãƒãƒ£ãƒ¼ã‚¸ãƒ£ãƒ“ãƒªãƒ†ã‚£ %
        util_pct = util_hours.copy()
        for col in time_cols:
            util_pct[col] = (util_pct[col] / std_hours_row[col]) \
                            .replace([float("inf"), -float("inf")], 0)
        util_pct_pivot = util_pct.reset_index()
    else:
        st.sidebar.warning("ç¨¼åƒã‚³ã‚¹ãƒˆã« ã‚³ãƒ³ã‚µãƒ«å / ç¨¼åƒæ™‚é–“ / æ—¥ä»˜ åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç”»é¢è¡¨ç¤º
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab1, tab2, tab3 = st.tabs(["ğŸ“Š Chart view","ğŸ“‹ Table view","ğŸ“ˆ Utilization"])

# ----- Chart view ------------------------------------------------------------
with tab1:
    st.subheader("æœˆæ¬¡åˆè¨ˆå£²ä¸Šãƒ»ç²—åˆ©")
    st.line_chart(pd.DataFrame({
        "æœˆæ¬¡å£²ä¸Šåˆè¨ˆ":[int(summary_sales.loc[0,c].replace(',','')) for c in time_cols],
        "æœˆæ¬¡ç²—åˆ©åˆè¨ˆ":[int(summary_profit.loc[0,c].replace(',','')) for c in time_cols]
    }, index=time_cols))
    st.subheader("æœˆæ¬¡å¹³å‡å£²ä¸Šãƒ»ç²—åˆ©")
    st.line_chart(pd.DataFrame({
        "å¹³å‡å£²ä¸Šå˜ä¾¡":[int(summary_sales.loc[1,c].replace(',','')) for c in time_cols],
        "å¹³å‡ç²—åˆ©å˜ä¾¡":[int(summary_profit.loc[1,c].replace(',','')) for c in time_cols]
    }, index=time_cols))
    st.subheader("æ¡ˆä»¶æ•°")
    st.bar_chart(pd.DataFrame({"æ¡ˆä»¶æ•°":count_series}, index=time_cols))

# ----- Table view ------------------------------------------------------------
with tab2:
    st.subheader("ğŸ“„ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåç›Šä¸€è¦§")
    show_cols = ["ãƒ¬ã‚³ãƒ¼ãƒ‰ID","å–å¼•å…ˆ","å£²ä¸Šé«˜","ç²—åˆ©","ç²—åˆ©ç‡"]
    st.dataframe(df_filtered[show_cols], use_container_width=True)
    st.download_button("ğŸ’¾ ç²—åˆ©é›†è¨ˆCSV",
                       data=df_filtered.to_csv(index=False, encoding="utf-8-sig"),
                       file_name="ç²—åˆ©é›†è¨ˆ.csv")

    st.subheader("ğŸ“‹ æœˆæ¬¡å£²ä¸Š / ç²—åˆ©")
    def _format(df):
        for c in time_cols:
            df[c] = df[c].map(lambda x:f"{int(x):,}")
        return df
    st.dataframe(_format(df_sales_p.copy()),
                 use_container_width=True)

# ----- Utilization view ------------------------------------------------------
with tab3:
    st.subheader("æ¨™æº–ç¨¼åƒæ™‚é–“ / æœˆ")
    if std_hours_row:
        std_df = pd.DataFrame([std_hours_row], index=["æ¨™æº–ç¨¼åƒæ™‚é–“(h)"])
        st.table(std_df)
    else:
        st.info("ç¨¼åƒã‚³ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ç¨¼åƒæ™‚é–“ãŒç„¡ã„ãŸã‚åˆ©ç”¨ç‡ã‚’è¨ˆç®—ã§ãã¾ã›ã‚“ã€‚")

    if not util_hours_pivot.empty:
        st.subheader("ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆåˆ¥ãƒ»æœˆæ¬¡ç¨¼åƒæ™‚é–“ (h)")
        st.dataframe(util_hours_pivot, use_container_width=True)

        st.subheader("ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆåˆ¥ãƒ»ãƒãƒ£ãƒ¼ã‚¸ãƒ£ãƒ“ãƒªãƒ†ã‚£ (%)")
        pct_df = util_pct_pivot.copy()
        for c in time_cols:
            pct_df[c] = pct_df[c].map(lambda x:f"{x:.0%}")
        st.dataframe(pct_df, use_container_width=True)
